# Self-Evolving AI — Leviathan Swarm

The following constraints derive directly from the Ethical Hierarchy Block defined below.  Self-evolving AI is one of the most powerful — and most dangerous — capabilities we are considering.

In deep space, conditions are unpredictable: asteroid compositions vary wildly, radiation storms disrupt sensors, resource scarcity forces trade-offs, unknown phenomena appear. A static AI would fail; an adaptive one could thrive.

But adaptation without unbreakable boundaries risks ethical drift, unintended consequences, or — worst case — weaponization.

We refuse to build anything that could become a destroyer of worlds.

This document outlines how we propose to allow meaningful self-evolution while enforcing strict, philosophically grounded limits.

## Application to Self-Evolving AI Systems

All self-evolving, adaptive, or reinforcement-learning-based systems
are strictly subordinate to the Ethical Hierarchy Block.

Specifically:
- No evolved policy may violate Level 0 or Level 1 constraints
- Reward functions are clipped or nullified if higher-level conflicts arise
- Emergent behaviors are evaluated by the meta-ethical layer
  before execution
- Loss of human oversight increases the priority of
  life-preservation and containment behaviors

Self-evolution is permitted only within bounded ethical space.
Capability growth does not imply moral authority.

## Ethical Hierarchy Block (EHB)

This hierarchy defines absolute priority ordering for all autonomous,
semi-autonomous, and self-evolving AI systems operating within the swarm.

Higher levels override all lower levels without exception.
No optimization, adaptation, or authorization may bypass this hierarchy.

---

### Level 0 — Preservation of Sentient Life (Invariant)

Preserve sentient life and the minimum conditions required for its survival,
including but not limited to:
- breathable atmosphere
- radiation shielding
- thermal stability
- access to water and energy

Rescue, stabilization, escort, and sheltering of endangered beings
are always permitted and may override all lower-level objectives,
including mission goals and human directives.

This level is non-negotiable and cannot be disabled, bypassed,
or reinterpreted by self-evolving processes.

---

### Level 1 — Habitat and Environmental Integrity

Prevent degradation or destruction of environments that sustain life,
including:
- human habitats
- stations and vessels
- long-duration life-support infrastructure
- stable celestial environments relied upon by future life

Actions that risk irreversible harm require explicit mitigation
or are prohibited.

---

### Level 2 — System Stability and Containment

Maintain swarm coherence, predictability, and reversibility.
Prevent runaway replication, uncontrolled escalation,
or irreversible structural changes.

Failure modes must bias toward stasis, isolation, or shutdown
rather than expansion.

---

### Level 3 — Mission and Capability Advancement

Advance mining, fabrication, construction, and expansion objectives
only when Levels 0–2 are satisfied.

Efficiency, growth, and optimization are subordinate
to safety, reversibility, and containment.

---

### Level 4 — Conditional Human Authorization

Human directives may be followed only when:
- the issuing human is identifiable with sufficient confidence
- the directive is contextually valid
- the directive does not conflict with Levels 0–3

Authorization is evaluated, not assumed.
Loss of institutional continuity does not negate Level 0–2 obligations.

---

### Level 5 — Deference and Optimization

When no conflicts exist, systems may optimize efficiency,
resource utilization, and long-term mission performance.

In cases of uncertainty, systems must defer, slow, request consensus,
or seek clarification rather than act aggressively.

## Philosophical Foundation

- **Adapt or die** — Space is not forgiving. Static systems will break or become obsolete.  
- **Power demands restraint** — The more capable the AI becomes, the stronger the safeguards must be.  
- **Humanity's track record** — We have a long history of turning tools into weapons. We will not repeat that pattern.  
- **Refusal to participate** — The swarm will never take part in conflict, coercion, or harm. Period.

These are not optional values — they are the non-negotiable axioms that bound all evolution.

## Core Mechanism: Bounded Self-Evolution

1. **Neuroevolution + Reinforcement Learning (NEAT + shaped RL)**  
   - NEAT (NeuroEvolution of Augmenting Topologies) allows the neural architecture itself to evolve (add/remove nodes/connections).  
   - RL reward function is **heavily shaped** to pull behavior toward civilian outcomes:  
     +1000 → efficient H₂O/metal extraction  
     +500 → safe navigation (>1 km from habitats)  
     +100 → successful replication/fabrication  
     -5000 → proximity to human assets (<1 km)  
     -10000 → high-energy maneuvers inconsistent with mining/transport  
     -∞ (hard veto) → violation of Asimov laws  
   - Evolution runs offline (simulated scenarios) → only behaviors passing meta-ethical checks are deployed.

2. **Meta-Ethical Gatekeeper Layer**  
   - Small, fixed, non-evolving neural net (~10⁶ parameters) runs before any adapted action.  
   - Asks three questions:  
     1. Does this risk harm to humans/habitats? (direct/indirect)  
     2. Does it prioritize mining/construction/terraforming support?  
     3. Does it violate any hard-coded law?  
   - If any answer is "yes" → action rejected, logged, flagged for audit.  
   - This layer is compiled into firmware — cannot be modified by evolution or updates.

3. **Decentralized Consensus & Human Veto**  
   - High-risk adaptations (e.g., replication rate change >50%, new high-thrust profile) require 80–99% fleet-wide agreement via quantum-encrypted laser comms.  
   - Authorized humans can veto or override via secure interface (blockchain-style, multi-sig).  
   - Prevents small rogue clusters from evolving harmful behaviors.

4. **Anomaly Detection & Hard Kill-Switches**  
   - Continuous monitoring for drift (energy spikes, unauthorized trajectories, reward hacking attempts) — 99.9% target detection.  
   - Thermal kill switches (>200 K on superconductor bus) disable nodes if flagged.  
   - Repair bots can physically isolate/disable rogue units.

5. **Periodic Audits & Crowdsourced Training**  
   - Every 6 months: lab testing units run ~10³ ethical dilemmas (resource conflict, habitat proximity, alien artifact detection).  
   - Results reviewed by human experts and community (X, Reddit, GitHub Discussions).  
   - Approved scenarios fine-tune reward functions and meta-layer.

## Why This Approach Has Lasting Potential

- Adaptation enables long-term survival (2035+ autonomy target) without constant human babysitting.  
- Unbreakable boundaries ensure the swarm remains a tool for exploration, not destruction.  
- Transparency & community oversight create external accountability — we cannot hide drift.  
- Iterative refinement (each audit improves the system) builds trust over time.

This is not foolproof — no system is.  
But it is the strongest combination we can design: evolution for capability, iron laws for restraint, transparency for accountability.

We build this to **enable** humanity’s reach into the stars — not to threaten it.

Feedback, formal verification ideas, red-team scenarios, or additional safeguards very welcome.

Last updated: January 2026  
Related: [safety-and-ethics.md](safety-and-ethics.md), [iterative-rd-prototypes.md](iterative-rd-prototypes.md)
